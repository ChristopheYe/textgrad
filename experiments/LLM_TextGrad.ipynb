{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/bioel/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-12 15:53:17 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n",
      "WARNING 09-12 15:53:17 cuda.py:69] Detected different devices in the system: \n",
      "WARNING 09-12 15:53:17 cuda.py:69] Tesla V100-PCIE-32GB\n",
      "WARNING 09-12 15:53:17 cuda.py:69] NVIDIA A40\n",
      "WARNING 09-12 15:53:17 cuda.py:69] NVIDIA A40\n",
      "WARNING 09-12 15:53:17 cuda.py:69] NVIDIA A40\n",
      "WARNING 09-12 15:53:17 cuda.py:69] Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 15:53:17,530\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "import ujson\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "from bioel.utils.umls_utils import UmlsMappings\n",
    "from bioel.utils.bigbio_utils import CUIS_TO_REMAP, CUIS_TO_EXCLUDE, DATASET_NAMES, VALIDATION_DOCUMENT_IDS\n",
    "from bioel.utils.bigbio_utils import load_bigbio_dataset, add_deabbreviations, load_dataset_df, dataset_to_documents, dataset_to_df, load_dataset_df, resolve_abbreviation, dataset_unique_tax_ids\n",
    "from bioel.utils.solve_abbreviation.solve_abbreviations import create_abbrev\n",
    "\n",
    "from bioel.ontology import BiomedicalOntology\n",
    "from bioel.models.arboel.biencoder.data.data_utils import process_ontology\n",
    "from bioel.evaluate import Evaluate\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import argparse\n",
    "import concurrent\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import textgrad as tg\n",
    "from textgrad.tasks import load_task\n",
    "import random\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import openai\n",
    "import json\n",
    "from ids import open_ai_api_key\n",
    "openai.api_key = open_ai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_ai_api_key\n",
    "import re\n",
    "import logging\n",
    "from collections import Counter, defaultdict\n",
    "from utils_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" \n",
    "sampling_params = SamplingParams(temperature=0, top_p=0.9, max_tokens=20, stop=[\"<|eot_id|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_dir = \"/mitchell/entity-linking/kbs/medic.tsv\"\n",
    "name = \"medic\"\n",
    "ontology2 = BiomedicalOntology.load_medic(filepath=ontology_dir, name=name)\n",
    "\n",
    "# entrez_dict = {\"name\" : \"entrez\",\n",
    "#              \"filepath\" : \"/mitchell/entity-linking/el-robustness-comparison/data/gene_info.tsv\",\n",
    "#              \"dataset\" : \"gnormplus\",}\n",
    "# ontology = BiomedicalOntology.load_entrez(**entrez_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ncbi_disease'\n",
    "# dataset_name = 'gnormplus'\n",
    "path_to_abbrev = \"/home2/cye73/data_test2/abbreviations.json\"\n",
    "dataset = load_bigbio_dataset(dataset_name)\n",
    "dataset = add_deabbreviations(dataset, path_to_abbrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_to_df(dataset)\n",
    "test_df = dataset_df[dataset_df['split'] == 'test']\n",
    "train_df = dataset_df[dataset_df['split'] == 'train']\n",
    "# test_df\n",
    "docs = dataset_to_documents(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sample(item, eval_fn, model):\n",
    "    \"\"\"\n",
    "    This function allows us to evaluate if an answer to a question in the prompt is a good answer.\n",
    "\n",
    "    \"\"\"\n",
    "    x, y = item\n",
    "    x = tg.Variable(x, requires_grad=False, role_description=\"query to the language model\")\n",
    "    y = tg.Variable(y, requires_grad=False, role_description=\"correct answer for the query\")\n",
    "    response = model(x)\n",
    "    try:\n",
    "        eval_output_variable = eval_fn(inputs=dict(prediction=response, ground_truth_answer=y))\n",
    "        return int(eval_output_variable.value)\n",
    "    except:\n",
    "        eval_output_variable = eval_fn([x, y, response])\n",
    "        eval_output_parsed = eval_fn.parse_output(eval_output_variable)\n",
    "        return int(eval_output_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dataset(test_set, eval_fn, model, max_samples: int=None):\n",
    "    if max_samples is None:\n",
    "        max_samples = len(test_set)\n",
    "    accuracy_list = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        futures = []\n",
    "        for _, sample in enumerate(test_set):\n",
    "            \n",
    "            future = executor.submit(eval_sample, sample, eval_fn, model)\n",
    "            futures.append(future)\n",
    "            if len(futures) >= max_samples:\n",
    "                break\n",
    "        tqdm_loader = tqdm(concurrent.futures.as_completed(futures), total=len(futures), position=0)\n",
    "        for future in tqdm_loader:\n",
    "            acc_item = future.result()\n",
    "            accuracy_list.append(acc_item)\n",
    "            tqdm_loader.set_description(f\"Accuracy: {np.mean(accuracy_list)}\")\n",
    "    return accuracy_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation_revert(system_prompt: tg.Variable, results, model, eval_fn, val_set):\n",
    "    val_performance = np.mean(eval_dataset(val_set, eval_fn, model))\n",
    "    previous_performance = np.mean(results[\"validation_acc\"][-1])\n",
    "    print(\"val_performance: \", val_performance)\n",
    "    print(\"previous_performance: \", previous_performance)\n",
    "    previous_prompt = results[\"prompt\"][-1]\n",
    "    \n",
    "    if val_performance < previous_performance:\n",
    "        print(f\"rejected prompt: {system_prompt.value}\")\n",
    "        system_prompt.set_value(previous_prompt)\n",
    "        val_performance = previous_performance\n",
    "\n",
    "    results[\"validation_acc\"].append(val_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-12 15:53:51 config.py:1651] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 09-12 15:53:51 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=25000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 09-12 15:53:51 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-12 15:53:51 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/cye73/conda_envs/bioel/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/nethome/cye73/conda_envs/bioel/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 15:54:01 model_runner.py:915] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 09-12 15:54:01 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-12 15:54:01 selector.py:116] Using XFormers backend.\n",
      "INFO 09-12 15:54:01 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.86s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.13s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.44s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.62s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.55s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 15:54:09 model_runner.py:926] Loading model weights took 14.9888 GB\n",
      "INFO 09-12 15:54:13 gpu_executor.py:122] # GPU blocks: 11158, # CPU blocks: 2048\n",
      "INFO 09-12 15:54:16 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-12 15:54:16 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-12 15:54:36 model_runner.py:1335] Graph capturing finished in 20 secs.\n"
     ]
    }
   ],
   "source": [
    "set_seed(12)\n",
    "# Set CUDA devices to see both GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "llm_api_test = tg.get_engine(engine_name=\"vllm-meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                             dtype='half', \n",
    "                             enforce_eager=True, \n",
    "                             gpu_memory_utilization=0.6\n",
    "                             )\n",
    "\n",
    "llm_api_eval = tg.get_engine(engine_name=\"gpt-4o-mini\", \n",
    "                             dtype='half', \n",
    "                             enforce_eager=True, \n",
    "                             gpu_memory_utilization=0.6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test Set Lengths:  50 100 100\n"
     ]
    }
   ],
   "source": [
    "tg.set_backward_engine(llm_api_eval, override=True)\n",
    "\n",
    "# Load the data and the evaluation function\n",
    "train_set, val_set, test_set, eval_fn = load_task(\"BBH_object_counting\", evaluation_api=llm_api_eval)\n",
    "print(\"Train/Val/Test Set Lengths: \", len(train_set), len(val_set), len(test_set))\n",
    "STARTING_SYSTEM_PROMPT = train_set.get_task_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Value must be a string, int, or image (bytes). Got: <class 'numpy.int64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tg\u001b[38;5;241m.\u001b[39mTextualGradientDescent(engine\u001b[38;5;241m=\u001b[39mllm_api_eval, parameters\u001b[38;5;241m=\u001b[39m[system_prompt])\n\u001b[1;32m     17\u001b[0m results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n\u001b[0;32m---> 18\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43meval_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(eval_dataset(val_set, eval_fn, model))\n\u001b[1;32m     20\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(system_prompt\u001b[38;5;241m.\u001b[39mget_value())\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36meval_dataset\u001b[0;34m(test_set, eval_fn, model, max_samples)\u001b[0m\n\u001b[1;32m     13\u001b[0m tqdm_loader \u001b[38;5;241m=\u001b[39m tqdm(concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(futures), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(futures), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm_loader:\n\u001b[0;32m---> 15\u001b[0m     acc_item \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     accuracy_list\u001b[38;5;241m.\u001b[39mappend(acc_item)\n\u001b[1;32m     17\u001b[0m     tqdm_loader\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(accuracy_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/nethome/cye73/conda_envs/bioel/lib/python3.9/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/nethome/cye73/conda_envs/bioel/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/nethome/cye73/conda_envs/bioel/lib/python3.9/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36meval_sample\u001b[0;34m(item, eval_fn, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m x, y \u001b[38;5;241m=\u001b[39m item\n\u001b[1;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m tg\u001b[38;5;241m.\u001b[39mVariable(x, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, role_description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery to the language model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrole_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorrect answer for the query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/home2/cye73/llm_disambiguator/textgrad/textgrad/variable.py:43\u001b[0m, in \u001b[0;36mVariable.__init__\u001b[0;34m(self, value, image_path, predecessors, requires_grad, role_description)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m requires_grad) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(_predecessor_requires_grad) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the variable does not require grad, none of its predecessors should require grad.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn this case, following predecessors require grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_predecessor_requires_grad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(value) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue must be a string, int, or image (bytes). Got: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(value))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     45\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(value)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Value must be a string, int, or image (bytes). Got: <class 'numpy.int64'>"
     ]
    }
   ],
   "source": [
    "train_loader = tg.tasks.DataLoader(train_set, batch_size=3, shuffle=True)\n",
    "\n",
    "\n",
    "# Testing the 0-shot performance of the evaluation engine\n",
    "system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT, \n",
    "                            requires_grad=True, \n",
    "                            role_description=\"system prompt to the language model\")\n",
    "model_evaluation = tg.BlackboxLLM(llm_api_eval, system_prompt)\n",
    "\n",
    "system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT, \n",
    "                            requires_grad=True,\n",
    "                            role_description=\"structured system prompt to a somewhat capable language model that specifies the behavior and strategies for the QA task\")\n",
    "model = tg.BlackboxLLM(llm_api_test, system_prompt)\n",
    "\n",
    "optimizer = tg.TextualGradientDescent(engine=llm_api_eval, parameters=[system_prompt])\n",
    "\n",
    "results = {\"test_acc\": [], \"prompt\": [], \"validation_acc\": []}\n",
    "results[\"test_acc\"].append(eval_dataset(test_set, eval_fn, model))\n",
    "results[\"validation_acc\"].append(eval_dataset(val_set, eval_fn, model))\n",
    "results[\"prompt\"].append(system_prompt.get_value())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    for steps, (batch_x, batch_y) in enumerate((pbar := tqdm(train_loader, position=0))):\n",
    "        pbar.set_description(f\"Training step {steps}. Epoch {epoch}\")\n",
    "        optimizer.zero_grad()\n",
    "        losses = []\n",
    "        for (x, y) in zip(batch_x, batch_y):\n",
    "            x = tg.Variable(x, requires_grad=False, role_description=\"query to the language model\")\n",
    "            y = tg.Variable(y, requires_grad=False, role_description=\"correct answer for the query\")\n",
    "            response = model(x)\n",
    "            try:\n",
    "                eval_output_variable = eval_fn(inputs=dict(prediction=response, ground_truth_answer=y))\n",
    "            except:\n",
    "                eval_output_variable = eval_fn([x, y, response])\n",
    "            losses.append(eval_output_variable)\n",
    "        total_loss = tg.sum(losses)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        run_validation_revert(system_prompt, results, model, eval_fn, val_set)\n",
    "        \n",
    "        print(\"sys prompt: \", system_prompt)\n",
    "        test_acc = eval_dataset(test_set, eval_fn, model)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        results[\"prompt\"].append(system_prompt.get_value())\n",
    "        if steps == 3:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
